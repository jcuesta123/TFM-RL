{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0Mn1KxOeDNJHhWk2PQDSI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcuesta123/TFM-RL/blob/main/airplane_climb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTS\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "G8vlwjQWLkNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#General parametes\n",
        "m_a = 1000 # kg\n",
        "g = 9.81   # m/s2\n",
        "l_a = 4 # m airplane length\n",
        "S_w = 4 # m2 wing surface\n",
        "S_t = S_w/4 # m2 tail wing surface\n",
        "l_w = 0.5 # m wing_length\n",
        "I_a = (1/12) * m_a * (l_a)**2\n",
        "ro = 1 # kg/m3  air density\n",
        "pi = math.pi\n",
        "\n",
        "#State variables\n",
        "x = 0       # m\n",
        "z = 100     # m\n",
        "v = 100     # m/s\n",
        "gamma = 0   # rad\n",
        "theta = 0   # rad\n",
        "v_theta = 0 # rad/s\n",
        "T = 1500 # N  thrust\n",
        "EC = 0 # elevator control\n",
        "\n",
        "alpha = theta - gamma\n",
        "\n",
        "#Calculation values\n",
        "# abajo  deltaT = 0.1 # s"
      ],
      "metadata": {
        "id": "J9YQ8mU5LnCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = np.array([x, z, v, gamma, theta, v_theta, T, EC])\n",
        "\n",
        "# gamma = angle between axis x and v_airplane\n",
        "# theta = angle between axis x and x_airplane\n",
        "# alpha = theta - gamma (attack angle, how wind impacts on wings)\n",
        "# T = thrust in N\n",
        "\n",
        "print(state)\n",
        "print(alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyBoS2TJLpGN",
        "outputId": "6340dc4f-c6a3-41e1-afdb-8bdd074a17e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0  100  100    0    0    0 1500    0]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LIFT function\n",
        "# v_tipic = 100 m/s,  5º or 0.087 rad -> 10 kN\n",
        "\n",
        "C_L_alpha = 5.73 # coeficcient increase in C_L per radian, = 1CL per 10º\n",
        "max_C_L_angle = 0.26 #rad, about 15º\n",
        "C_L_alpha_zero = 0 #\n",
        "\n",
        "def L(v,alpha, EC):\n",
        "\n",
        "  # OJO al entorno\n",
        "\n",
        "  lift_w = 0.5 * ro * v**2 * S_w * (0.25 + 2.87 * alpha)\n",
        "  lift_t = 0.5 * ro * v**2 * S_t * (0.00 + 5.73 * (alpha + EC))\n",
        "  lift = lift_w + lift_t\n",
        "\n",
        "  return lift, lift_w, lift_t"
      ],
      "metadata": {
        "id": "RWfgaPdML4fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(L(100,0.087,0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQYNhjgUL8X9",
        "outputId": "ac03f27e-8df4-4e21-fc6e-b615f0014479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12486.349999999999, 9993.8, 2492.55)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def D(v,alpha, EC):\n",
        "\n",
        "  C_D_w = 0.05 + 0.089 * (0.25 + 2.87 * alpha) **2\n",
        "  C_D_t = 0.05 + 0.089 * (0.00 + 5.73 * (alpha + EC)) **2\n",
        "\n",
        "  drag = 0.5 * ro * v**2 * (S_w * C_D_w + S_t * C_D_t)\n",
        "\n",
        "  return drag"
      ],
      "metadata": {
        "id": "yp9GM7esMJC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(D(100,0.087,0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhMTXIWAMNz2",
        "outputId": "3401e0cd-36d3-412b-c9e0-236276a280cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1805.0363090025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot graphics\n",
        "\n",
        "def plot_traj(matrix_state):\n",
        "\n",
        "  x = matrix_state[:, 0]\n",
        "  z = matrix_state[:, 1]\n",
        "  v = matrix_state[:, 2]\n",
        "  gamma = matrix_state[:, 3]\n",
        "  theta = matrix_state[:, 4]\n",
        "  alpha = theta - gamma\n",
        "  v_theta = matrix_state[:, 5]\n",
        "  T = matrix_state[:, 6]\n",
        "  EC = matrix_state[:, 7]\n",
        "  t = matrix_state[:, 8]\n",
        "  ep_reward = matrix_state[:, 9]\n",
        "  v_z = matrix_state[:, 2] * np.sin(matrix_state[:, 3])\n",
        "\n",
        "  plt.subplot(4, 2, 1)  # 2 filas, 1 columna, primera posición\n",
        "  plt.plot(x, z)  # 'r--' indica línea roja discontinua\n",
        "  plt.title('Trayectory (m)')\n",
        "  plt.xlabel('Axis X')\n",
        "  plt.ylabel('Axis Y')\n",
        "  plt.axhline(0, color='black', linewidth=0.5)\n",
        "  plt.axvline(0, color='black', linewidth=0.5)\n",
        "\n",
        "  plt.subplot(4, 2, 2)  # 2 filas, 1 columna, primera posición\n",
        "  plt.plot(t, v,'-.')  # 'r--' indica línea roja discontinua\n",
        "  plt.title('Absolute Velocity (m/s)')\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.subplot(4, 2, 3)  # 2 filas, 1 columna, primera posición\n",
        "  plt.plot(t, v_theta*180/pi,'-.')  # 'r--' indica línea roja discontinua\n",
        "  plt.title('Angular velocity (º/s)')\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.subplot(4, 2, 4)  # 2 filas, 1 columna, primera posición\n",
        "  plt.plot(t, gamma*180/pi, label='gamma (red)', color='red')  # 'r--' indica línea roja discontinua\n",
        "  plt.plot(t, theta*180/pi, label='theta (green)', color='green')  # 'r--' indica línea roja discontinua\n",
        "  plt.plot(t, alpha*180/pi, label='alpha (blue)', color='blue')  # 'r--' indica línea roja discontinua\n",
        "  plt.legend()\n",
        "  plt.title('Angles (º)')\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.subplot(4, 2, 5)  # 2 filas, 1 columna, primera posición\n",
        "  plt.plot(t, T)  # 'r--' indica línea roja discontinua\n",
        "  plt.title('T (N)')\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.subplot(4, 2, 6)  # 2 filas, 1 columna, primera posición\n",
        "  plt.plot(t, EC*180/pi)  # 'r--' indica línea roja discontinua\n",
        "  plt.title('EC (º)')\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.subplot(4, 2, 7)  # 2 filas, 1 columna, primera posición\n",
        "  plt.plot(t, ep_reward)  # 'r--' indica línea roja discontinua\n",
        "  plt.title('Ep. reward')\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.subplot(4, 2, 8)  # 2 filas, 1 columna, primera posición\n",
        "  plt.plot(t, v_z)  # 'r--' indica línea roja discontinua\n",
        "  plt.title('Vertical speed (m/s)')\n",
        "  plt.grid(True)\n",
        "\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "RUrKuyr7MXft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T_range = np.array([0, 0.3])*m_a*g   #trust up to 30% of weight\n",
        "EC_range = np.array([-3, 3])*pi/180  # EC from -3º to 3º  ###OJO grados\n",
        "\n",
        "print(T_range,EC_range)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs3YrcCiMaL2",
        "outputId": "d1ff8767-56d1-496a-b0e6-d3411cdf2130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0. 2943.] [-0.05235988  0.05235988]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#environment\n",
        "\n",
        "def environment(state, action):\n",
        "\n",
        "  x,z,v,gamma,theta,v_theta,T, EC = state\n",
        "  alpha = theta - gamma #####   #########   #####  ###########  OJO al orden\n",
        "  lift, lift_w, lift_t = L(v,alpha,EC)\n",
        "  drag = D(v,alpha,EC)\n",
        "\n",
        "  #PROCESSING ACTION\n",
        "  if action == 1 and T <= T_range[1]:\n",
        "    T += T_range[1]/10\n",
        "  elif action == 2 and T >= T_range[0]:\n",
        "    T -= T_range[1]/10\n",
        "  elif action == 3 and EC <= EC_range[1]:\n",
        "    EC += EC_range[1]/10\n",
        "  elif action == 4 and EC >= EC_range[0]:\n",
        "    EC -= EC_range[1]/10\n",
        "  else: #action == 0   do nothing\n",
        "    pass\n",
        "\n",
        "\n",
        "  #forces\n",
        "  a_x = (1/m_a) * (T * math.cos(theta) - drag * math.cos(gamma) - lift * math.sin(theta))\n",
        "  a_z = (1/m_a) * (T * math.sin(theta) - drag * math.sin(gamma) + lift * math.cos(theta) - m_a * g)\n",
        "  a_theta = (1/I_a) * (l_w * lift_w - l_t * lift_t)\n",
        "\n",
        "  v_x = v * math.cos(gamma) + a_x * deltaT\n",
        "  v_z = v * math.sin(gamma) + a_z * deltaT\n",
        "  v_theta = v_theta + a_theta * deltaT\n",
        "\n",
        "  x = x + v_x * deltaT\n",
        "  z = z + v_z * deltaT\n",
        "  theta = theta + v_theta * deltaT\n",
        "\n",
        "  v = math.sqrt(v_x ** 2 + v_z ** 2)\n",
        "  gamma = math.atan2(v_z, v_x)\n",
        "\n",
        "  next_state = np.array([x, z, v, gamma, theta, v_theta, T, EC])\n",
        "\n",
        "  # call reward function\n",
        "  reward = reward_function(next_state, done)\n",
        "\n",
        "  return next_state, reward, done"
      ],
      "metadata": {
        "id": "J9XVP-WDMo3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_function(state, done):\n",
        "\n",
        "  reward = 0\n",
        "  reward += state[2] * state[3] * deltaT   # maximize v_z\n",
        "  reward += (state[1] - 1000)/100 * deltaT  # maximize altitute\n",
        "  if state[3] < 0 or state[4] < 0:  # penalise negative angles\n",
        "    reward += - deltaT\n",
        "\n",
        "  return reward"
      ],
      "metadata": {
        "id": "yDZZU2UuMram"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,z,v,gamma,theta,v_theta,T,EC = state"
      ],
      "metadata": {
        "id": "KMCownV2Mt6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = state.size\n",
        "num_actions = 5\n",
        "random.randint(0, num_actions-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cf7vhLrMxyd",
        "outputId": "253ea6d2-a6f8-4bc6-865e-098fe3d1b40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEURONAL NETWORKS\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"Dense neural network class.\"\"\"\n",
        "    def __init__(self, num_inputs, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_inputs, 64)\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.out = nn.Linear(128, num_actions)\n",
        "\n",
        "    def forward(self, states):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        x = F.relu(self.fc1(states))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.out(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "main_nn = DQN(num_features, num_actions).to(device)\n",
        "target_nn = DQN(num_features, num_actions).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(main_nn.parameters(), lr=1e-4)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEHhMMo4YUFZ",
        "outputId": "062c5cc2-93db-4ecb-fb54-12ba3a22abd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BUFFER MEMORY\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "  \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
        "  def __init__(self, size, device=\"cpu\"):\n",
        "    \"\"\"Initializes the buffer.\"\"\"\n",
        "    self.buffer = deque(maxlen=size)\n",
        "    self.device = device\n",
        "\n",
        "  def add(self, state, action, reward, next_state, done):\n",
        "    self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def sample(self, num_samples):\n",
        "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "    idx = np.random.choice(len(self.buffer), num_samples)\n",
        "    for i in idx:\n",
        "      elem = self.buffer[i]\n",
        "      state, action, reward, next_state, done = elem\n",
        "      states.append(np.array(state, copy=False))\n",
        "      actions.append(np.array(action, copy=False))\n",
        "      rewards.append(reward)\n",
        "      next_states.append(np.array(next_state, copy=False))\n",
        "      dones.append(done)\n",
        "    states = torch.as_tensor(np.array(states), device=self.device)\n",
        "    actions = torch.as_tensor(np.array(actions), device=self.device)\n",
        "    rewards = torch.as_tensor(np.array(rewards, dtype=np.float32), device=self.device\n",
        "    )\n",
        "    next_states = torch.as_tensor(np.array(next_states), device=self.device)\n",
        "    dones = torch.as_tensor(np.array(dones, dtype=np.float32), device=self.device)\n",
        "    return states, actions, rewards, next_states, dones"
      ],
      "metadata": {
        "id": "TfugUmstZEno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN FUNCTION\n",
        "\n",
        "def train_step(states, actions, rewards, next_states, dones):\n",
        "\n",
        "  max_next_qs = target_nn(next_states).max(-1).values\n",
        "  target = rewards + (1.0 - dones) * discount * max_next_qs\n",
        "  qs = main_nn(states)\n",
        "  action_masks = F.one_hot(actions, num_actions)\n",
        "  masked_qs = (action_masks * qs).sum(dim=-1)\n",
        "  loss = loss_fn(masked_qs, target.detach())\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss"
      ],
      "metadata": {
        "id": "wlzxwSrIZHMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ACTION\n",
        "\n",
        "def select_action(state, epsilon):\n",
        "\n",
        "  result = np.random.uniform()\n",
        "\n",
        "  if result < epsilon:\n",
        "    return random.randint(0, num_actions-1)   #random action\n",
        "  else:\n",
        "    qs = main_nn(state).cpu().data.numpy()    #NN action\n",
        "    return np.argmax(qs)"
      ],
      "metadata": {
        "id": "NhNp_9UgMzjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAINING LOOP\n",
        "\n",
        "# Hyperparameters.\n",
        "num_episodes = 200  #IMPORTANT\n",
        "epsilon = 1.0\n",
        "batch_size = 32\n",
        "discount = 0.99\n",
        "buffer = ReplayBuffer(100000, device=device)\n",
        "last_100_ep_rewards = []\n",
        "\n",
        "deltaT = 0.01\n",
        "l_t = l_w * 4\n",
        "\n",
        "initial_state = np.array([0, 1000, 88, 0, 0.087, 0, 2250, 0]) #x,z,v,gamma,theta,v_theta,T\n",
        "\n",
        "\n",
        "for episode in range(num_episodes+1):\n",
        "  state = initial_state.astype(np.float32)\n",
        "  cur_frame = 0\n",
        "  ep_reward = 0\n",
        "  done = False\n",
        "  time = 0\n",
        "\n",
        "  for i in range(3000):\n",
        "\n",
        "    state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
        "    action = select_action(state_in, epsilon)\n",
        "    next_state, reward, done = environment(state, action)\n",
        "    next_state = next_state.astype(np.float32)\n",
        "    ep_reward += reward\n",
        "\n",
        "    #save experience\n",
        "    buffer.add(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    cur_frame += 1\n",
        "\n",
        "    #copy main_nn weights to target_nn.\n",
        "    if cur_frame % 2000 == 0:\n",
        "      target_nn.load_state_dict(main_nn.state_dict())\n",
        "\n",
        "    #train neural network.\n",
        "    if len(buffer) > batch_size:\n",
        "      states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "      loss = train_step(states, actions, rewards, next_states, dones)\n",
        "\n",
        "  #epsilon decresases each episode\n",
        "  if epsilon > 0.05:\n",
        "    epsilon -= 0.95/num_episodes\n",
        "\n",
        "  if len(last_100_ep_rewards) == 100:\n",
        "    last_100_ep_rewards = last_100_ep_rewards[1:]\n",
        "  last_100_ep_rewards.append(ep_reward)\n",
        "\n",
        "  #printing training progress\n",
        "  if episode % 10 == 0:\n",
        "    print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}.'\n",
        "          f' Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.2f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wavW-flSelkR",
        "outputId": "9a4e624c-06ca-4067-d868-a801e2a4f562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/200. Epsilon: 0.995. Reward in last 100 episodes: -482.04\n",
            "Episode 10/200. Epsilon: 0.948. Reward in last 100 episodes: -335.73\n",
            "Episode 20/200. Epsilon: 0.900. Reward in last 100 episodes: -286.85\n",
            "Episode 30/200. Epsilon: 0.853. Reward in last 100 episodes: -313.72\n",
            "Episode 40/200. Epsilon: 0.805. Reward in last 100 episodes: -321.66\n",
            "Episode 50/200. Epsilon: 0.758. Reward in last 100 episodes: -287.89\n",
            "Episode 60/200. Epsilon: 0.710. Reward in last 100 episodes: -289.49\n",
            "Episode 70/200. Epsilon: 0.663. Reward in last 100 episodes: -303.12\n",
            "Episode 80/200. Epsilon: 0.615. Reward in last 100 episodes: -304.26\n",
            "Episode 90/200. Epsilon: 0.568. Reward in last 100 episodes: -279.68\n",
            "Episode 100/200. Epsilon: 0.520. Reward in last 100 episodes: -249.32\n",
            "Episode 110/200. Epsilon: 0.473. Reward in last 100 episodes: -251.49\n",
            "Episode 120/200. Epsilon: 0.425. Reward in last 100 episodes: -268.80\n",
            "Episode 130/200. Epsilon: 0.378. Reward in last 100 episodes: -257.05\n",
            "Episode 140/200. Epsilon: 0.330. Reward in last 100 episodes: -249.78\n",
            "Episode 150/200. Epsilon: 0.283. Reward in last 100 episodes: -268.67\n",
            "Episode 160/200. Epsilon: 0.235. Reward in last 100 episodes: -267.56\n",
            "Episode 170/200. Epsilon: 0.188. Reward in last 100 episodes: -254.30\n",
            "Episode 180/200. Epsilon: 0.140. Reward in last 100 episodes: -242.27\n",
            "Episode 190/200. Epsilon: 0.093. Reward in last 100 episodes: -265.93\n",
            "Episode 200/200. Epsilon: 0.050. Reward in last 100 episodes: -278.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MAIN LOOP\n",
        "\n",
        "deltaT = 0.01\n",
        "l_t = l_w * 4\n",
        "\n",
        "\n",
        "\n",
        "initial_state = np.array([0, 1000, 88, 0, 0.087, 0, 2250, 0]) #x,z,v,gamma,theta,v_theta,T\n",
        "state = initial_state.astype(np.float32)\n",
        "\n",
        "time = 0\n",
        "ep_reward = 0\n",
        "done = False\n",
        "\n",
        "matrix_info = np.concatenate((state, [time], [ep_reward]))\n",
        "cur_frame = 0\n",
        "\n",
        "for i in range(9000):\n",
        "\n",
        "  state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
        "  action = select_action(state_in, 0)\n",
        "  next_state, reward, done = environment(state, action)\n",
        "  next_state = next_state.astype(np.float32)\n",
        "  ep_reward += reward\n",
        "\n",
        "  ep_reward += reward\n",
        "  time = deltaT * cur_frame\n",
        "\n",
        "  matrix_info = np.vstack([matrix_info, np.concatenate((state, [time],  [ep_reward]))])\n",
        "\n",
        "  state = next_state\n",
        "  cur_frame += 1\n",
        "\n",
        "print((state[4]-state[3])*180/pi)\n",
        "print(state[1])\n",
        "plot_traj(matrix_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "YQq2EbY0M5D9",
        "outputId": "2f5e86f3-73e8-460f-a0e6-0f963ca46d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'l_w' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8b5e9edbee61>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdeltaT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ml_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_w\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'l_w' is not defined"
          ]
        }
      ]
    }
  ]
}